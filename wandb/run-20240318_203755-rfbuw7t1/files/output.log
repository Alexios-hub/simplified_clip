Using the latest cached version of the module from /home/alex/.cache/huggingface/modules/datasets_modules/datasets/ydshieh--coco_dataset_script/e033205c0266a54c10be132f9264f2a39dcf893e798f6756d224b1ff5078998f (last modified on Thu Mar 14 12:38:10 2024) since it couldn't be found locally at ydshieh/coco_dataset_script., or remotely on the Hugging Face Hub.
Repo card metadata block was not found. Setting CardData to empty.
simplified transformer
CLIP(
  (visual): VisionTransformer(
    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)
    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
    (transformer): Transformer(
      (resblocks): Sequential(
        (0): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (1): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (2): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (3): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (4): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (5): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (6): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (7): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (8): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (9): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (10): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
        (11): ShapedAttentionBlock(
          (attn): ShapedAttention(
            (w_q): Linear(in_features=768, out_features=768, bias=True)
            (w_k): Linear(in_features=768, out_features=768, bias=True)
          )
          (ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Sequential(
            (c_fc): Linear(in_features=768, out_features=3072, bias=True)
            (gelu): QuickGELU()
            (c_proj): Linear(in_features=3072, out_features=768, bias=True)
          )
        )
      )
    )
    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  )
  (transformer): Transformer(
    (resblocks): Sequential(
      (0): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (1): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (2): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (3): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (4): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (5): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (6): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (7): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (8): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (9): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (10): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
      (11): ShapedAttentionBlock(
        (attn): ShapedAttention(
          (w_q): Linear(in_features=512, out_features=512, bias=True)
          (w_k): Linear(in_features=512, out_features=512, bias=True)
        )
        (ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (mlp): Sequential(
          (c_fc): Linear(in_features=512, out_features=2048, bias=True)
          (gelu): QuickGELU()
          (c_proj): Linear(in_features=2048, out_features=512, bias=True)
        )
      )
    )
  )
  (token_embedding): Embedding(49408, 512)
  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
)
Epoch 1/10:   0%|                                                                              | 0/1156 [00:00<?, ?it/s]











































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































































Epoch 1/10 Loss: 5.7891: 100%|██████████████████████████████████████████████████████| 1156/1156 [53:41<00:00,  2.79s/it]
Epoch 2/10:   0%|                                                                              | 0/1156 [00:00<?, ?it/s]

























































































































































































































































































Epoch 2/10 Loss: 6.0703:  24%|█████████████▍                                         | 282/1156 [13:21<41:25,  2.84s/it]
Traceback (most recent call last):
  File "/home/alex/simplified_clip/clip_torch.py", line 159, in <module>
    with tqdm(total=len(train_dataloader), desc=f"Epoch {epoch+1}/{args.num_epochs}") as pbar:
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File "/home/alex/simplified_clip/clip_torch.py", line 69, in __getitem__
    def __getitem__(self, index):
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 95, in __call__
    img = t(img)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torchvision/transforms/transforms.py", line 354, in forward
    return F.resize(img, self.size, self.interpolation, self.max_size, self.antialias)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torchvision/transforms/functional.py", line 467, in resize
    return F_pil.resize(img, size=output_size, interpolation=pil_interpolation)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py", line 250, in resize
    return img.resize(tuple(size[::-1]), interpolation)
  File "/home/alex/miniconda3/envs/sim_clip/lib/python3.10/site-packages/PIL/Image.py", line 2200, in resize
    return self._new(self.im.resize(size, resample, box))
KeyboardInterrupt